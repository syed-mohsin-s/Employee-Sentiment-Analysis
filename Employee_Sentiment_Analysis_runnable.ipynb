{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{},"source":["# Employee Sentiment Analysis — Runnable Notebook\n\nThis notebook is a cleaned, runnable version of the original Employee_Sentiment_Analysis.ipynb. It includes: data loading (you provide the CSV), preprocessing, TF-IDF vectorization, training a Logistic Regression classifier, evaluation, and saving the model.\n\nInstructions:\n- Put your CSV file (e.g. Employee_Reviews.csv) in a `data/` folder or update the `DATA_PATH` variable below.\n- Run the cells top-to-bottom.\n"]},{"cell_type":"code","metadata":{},"source":["# Optional: install common dependencies (uncomment if needed)\n# !pip install -q pandas scikit-learn matplotlib seaborn nltk joblib"]},{"cell_type":"code","metadata":{},"source":["import os\nimport re\nimport joblib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Download NLTK data if not present\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept LookupError:\n    nltk.download('stopwords')\n"]},{"cell_type":"code","metadata":{},"source":["# DATA_PATH: update this to point at your CSV file if different\nDATA_PATH = 'data/Employee_Reviews.csv'\n\nif not os.path.exists(DATA_PATH):\n    print(f\"Data file not found at {DATA_PATH}. Please place your CSV there or update DATA_PATH.\")\n    # create a tiny sample dataframe so the rest of the notebook runs for demo purposes\n    sample = {\n        'Review': [\"I love my job and the team is supportive.\", \"The management is poor and I am unhappy.\", \"Good work-life balance and fair pay.\"],\n        'Rating': [5, 1, 4]\n    }\n    df = pd.DataFrame(sample)\nelse:\n    df = pd.read_csv(DATA_PATH)\n\nprint('Columns:', df.columns.tolist())\nprint('Number of rows:', len(df))\n# show head\ndf.head()" ]},{"cell_type":"code","metadata":{},"source":["# Basic cleaning and label creation\n# Adjust column names used below if your CSV uses different ones\ntext_col = None\nif 'Review' in df.columns:\n    text_col = 'Review'\nelif 'review' in df.columns:\n    text_col = 'review'\nelif 'Text' in df.columns:\n    text_col = 'Text'\nelse:\n    # try first text-like column\n    for c in df.columns:\n        if df[c].dtype == object:\n            text_col = c\n            break\n\nif text_col is None:\n    raise RuntimeError('No text column detected in the dataset. Please update the notebook to point at your text column.')\n\n# Create label column 'sentiment' (binary: 1 positive, 0 negative)\nif 'sentiment' in df.columns.str.lower():\n    # user already has sentiment; try to find exact column\n    sentiment_cols = [c for c in df.columns if c.lower() == 'sentiment']\n    label_col = sentiment_cols[0]\n    df = df.rename(columns={label_col: 'sentiment'})\nelif 'Rating' in df.columns or 'rating' in df.columns:\n    rating_col = 'Rating' if 'Rating' in df.columns else 'rating'\n    # Map ratings to binary sentiment (>=3 -> positive)\n    df['sentiment'] = (df[rating_col].astype(float) >= 3).astype(int)\nelse:\n    # If there is no label, create a placeholder and ask user to label externally\n    df['sentiment'] = np.nan\n    print('No rating/sentiment column found — sentiment column created with NaNs. Please fill labels before training.')\n\n# Keep only rows with non-null text and sentiment\nbefore = len(df)\ndf = df[df[text_col].notna()]\nif df['sentiment'].isna().any():\n    print('Warning: some sentiment labels are missing. Remove or fill them before training.')\n\nprint(f'Removed {before - len(df)} rows with missing text. Remaining: {len(df)}')\n\n# simple text cleaning function\nstop_words = set(stopwords.words('english'))\n\ndef clean_text(s):\n    s = str(s).lower()\n    s = re.sub(r"http\S+|www\S+", "", s)\n    s = re.sub(r"[^a-z0-9\s]", " ", s)\n    s = re.sub(r"\s+", " ", s).strip()\n    tokens = [w for w in s.split() if w not in stop_words]\n    return ' '.join(tokens)\n\n# apply cleaning to a new column\nclean_col = 'text_clean'\ndf[clean_col] = df[text_col].astype(str).apply(clean_text)\n\n# show sample\ndf[[text_col, clean_col, 'sentiment']].head()" ]},{"cell_type":"code","metadata":{},"source":["# Prepare data for training (drop rows without labels)\ntrain_df = df[df['sentiment'].notna()].copy()\ntrain_df['sentiment'] = train_df['sentiment'].astype(int)\nX = train_df[clean_col].values\ny = train_df['sentiment'].values\n\nif len(X) < 5:\n    print('Not enough labeled examples to train a model. Add labels or provide a larger dataset.')\nelse:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    print('Train size:', len(X_train), 'Test size:', len(X_test))" ]},{"cell_type":"code","metadata":{},"source":["# Build a simple pipeline: TF-IDF -> Logistic Regression\nmodel = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1,2))),\n    ('clf', LogisticRegression(solver='liblinear', max_iter=1000))\n])\n\n# Train only if we have enough data\nif len(df[df['sentiment'].notna()]) >= 5:\n    model.fit(X_train, y_train)\n    print('Model trained.')\nelse:\n    print('Skipping training due to insufficient labeled data.')" ]},{"cell_type":"code","metadata":{},"source":["# Evaluation\nif len(df[df['sentiment'].notna()]) >= 5:\n    y_pred = model.predict(X_test)\n    print('Accuracy:', accuracy_score(y_test, y_pred))\n    print('\nClassification report:\n', classification_report(y_test, y_pred))\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\nelse:\n    print('No evaluation because model was not trained.')" ]},{"cell_type":"code","metadata":{},"source":["# Save model (if trained)\nOUT_DIR = 'models'\nos.makedirs(OUT_DIR, exist_ok=True)\nMODEL_PATH = os.path.join(OUT_DIR, 'sentiment_model.joblib')\nif 'model' in globals() and hasattr(model, 'predict') and len(df[df['sentiment'].notna()]) >= 5:\n    joblib.dump(model, MODEL_PATH)\n    print('Saved trained model to', MODEL_PATH)\nelse:\n    print('Model not saved (not trained).')"]}]}