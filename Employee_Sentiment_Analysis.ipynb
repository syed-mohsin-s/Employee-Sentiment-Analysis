{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOD1KJMHzOyUSRv/o9VTa8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syed-mohsin-s/Employee-Sentiment-Analysis/blob/main/Employee_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP5-vBP24L6e",
        "outputId": "aba036f7-64e7-4864-93f5-a56853c07dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (656, 13)\n",
            "Validation shape: (116, 13)\n",
            "Test shape: (225, 10)\n",
            "      id     person_name                                  nine_box_category  \\\n",
            "0      1        John Doe  Category 1: 'Risk' (Low performance, Low poten...   \n",
            "1  10045   Douglas Henry  Category 1: 'Risk' (Low performance, Low poten...   \n",
            "2  10044   Douglas Henry  Category 1: 'Risk' (Low performance, Low poten...   \n",
            "3  10005  Freddie Davies  Category 1: 'Risk' (Low performance, Low poten...   \n",
            "4  10004  Freddie Davies  Category 1: 'Risk' (Low performance, Low poten...   \n",
            "\n",
            "                                            feedback  adjusted  reviewed  \\\n",
            "0  John has not progressed in his position. He is...     False      True   \n",
            "1  Douglas Henry has been having trouble in all a...     False     False   \n",
            "2  Douglas has a lot to work on and areas to grow...     False     False   \n",
            "3  Freddie is a nice guy, but his performance and...     False      True   \n",
            "4  Freddie has been quite disappointing this quar...     False     False   \n",
            "\n",
            "   label  feedback_len  num_of_sent  performance_class  potential_class  \\\n",
            "0      0           287            5                  0                0   \n",
            "1      0           430            6                  0                0   \n",
            "2      0           290            4                  0                0   \n",
            "3      0           418            5                  0                0   \n",
            "4      0           449            4                  0                0   \n",
            "\n",
            "                                      feedback_clean data_type  \n",
            "0  john ha not progressed in his position he is c...     train  \n",
            "1  douglas henry ha been having trouble in all ar...     train  \n",
            "2  douglas ha a lot to work on and area to grow i...     train  \n",
            "3  freddie is a nice guy but his performance and ...     train  \n",
            "4  freddie ha been quite disappointing this quart...     train  \n",
            "label\n",
            "0    97\n",
            "4    88\n",
            "8    87\n",
            "3    80\n",
            "1    72\n",
            "2    70\n",
            "5    66\n",
            "7    65\n",
            "6    31\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv(\"train_set.csv\")\n",
        "val = pd.read_csv(\"validation_set.csv\")\n",
        "test = pd.read_csv(\"test_set.csv\")\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Validation shape:\", val.shape)\n",
        "print(\"Test shape:\", test.shape)\n",
        "\n",
        "print(train.head())\n",
        "print(train['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load training data\n",
        "train = pd.read_csv(\"train_set.csv\")\n",
        "val = pd.read_csv(\"validation_set.csv\")\n",
        "test = pd.read_csv(\"test_set.csv\")\n",
        "\n",
        "# Map numeric labels to text if needed\n",
        "# Update label_map to include all unique labels from the training data\n",
        "unique_labels = train['label'].unique()\n",
        "label_map = {label: f\"Category_{label}\" for label in unique_labels}\n",
        "train['sentiment'] = train['label'].map(label_map)\n",
        "val['sentiment'] = val['label'].map(label_map)\n",
        "\n",
        "# Use feedback_clean if available\n",
        "X_train = train['feedback_clean'].fillna(train['feedback'])\n",
        "X_val = val['feedback_clean'].fillna(val['feedback'])\n",
        "y_train = train['sentiment']\n",
        "y_val = val['sentiment']\n",
        "\n",
        "# TF-IDF feature extraction\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Validate\n",
        "y_pred = model.predict(X_val_tfidf)\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el9BkRq57so-",
        "outputId": "b18cc99b-ca8b-4dad-e72b-e544d3d37460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.46551724137931033\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Category_0       0.52      0.76      0.62        17\n",
            "  Category_1       0.44      0.31      0.36        13\n",
            "  Category_2       0.50      0.33      0.40        12\n",
            "  Category_3       0.48      0.71      0.57        14\n",
            "  Category_4       0.37      0.44      0.40        16\n",
            "  Category_5       0.60      0.25      0.35        12\n",
            "  Category_6       0.00      0.00      0.00         5\n",
            "  Category_7       0.50      0.18      0.27        11\n",
            "  Category_8       0.44      0.69      0.54        16\n",
            "\n",
            "    accuracy                           0.47       116\n",
            "   macro avg       0.43      0.41      0.39       116\n",
            "weighted avg       0.46      0.47      0.43       116\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test['feedback_clean'].fillna(test['feedback'])\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "test['predicted_sentiment'] = model.predict(X_test_tfidf)\n",
        "\n",
        "# Save for next tasks\n",
        "test[['id', 'person_name', 'feedback', 'predicted_sentiment']].to_csv(\"test_set_with_sentiment.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "n59834_58mBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 – Sentiment Labeling with a Transformer (DistilBERT)\n",
        "# -----------------------------------------------------------\n",
        "# This script trains a DistilBERT sequence classification model on your train/validation sets\n",
        "# and predicts sentiment for the test set. It is robust to both numeric and string labels\n",
        "# (e.g., 0/1/2 or \"Negative\"/\"Neutral\"/\"Positive\").\n",
        "#\n",
        "# Expected columns in train/validation:\n",
        "#   id, person_name, nine_box_category, feedback, adjusted, reviewed, label,\n",
        "#   feedback_len, num_of_sent, performance_class, potential_class,\n",
        "#   feedback_clean, data_type\n",
        "# Expected columns in test:\n",
        "#   same as above, but label may be absent.\n",
        "#\n",
        "# Outputs:\n",
        "#   - ./artifacts/\n",
        "#       ├── model/                          (saved best model + tokenizer)\n",
        "#       ├── metrics_validation.json         (eval metrics)\n",
        "#       ├── classification_report.txt       (per-class metrics on validation)\n",
        "#       ├── label_mapping.json              (id2label/label2id mapping)\n",
        "#       └── test_set_with_sentiment.csv     (predictions for test)\n",
        "#\n",
        "# Usage:\n",
        "#   - Put this file next to your CSVs, or adjust the file paths below.\n",
        "#   - Run in a notebook cell or as a script: `python task1_transformer_sentiment.py`\n",
        "#\n",
        "# Notes:\n",
        "#   - If you have a GPU available, training will be much faster. The script auto-detects CUDA.\n",
        "#   - You can switch to RoBERTa by changing MODEL_NAME to \"roberta-base\" and tokenizer/model classes accordingly.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "TRAIN_PATH = \"train_set.csv\"\n",
        "VAL_PATH   = \"validation_set.csv\"\n",
        "TEST_PATH  = \"test_set.csv\"\n",
        "\n",
        "TEXT_COL_PRIMARY = \"feedback_clean\"  # preferred clean text\n",
        "TEXT_COL_FALLBACK = \"feedback\"       # fallback if clean text missing/empty\n",
        "LABEL_COL = \"label\"\n",
        "ID_COL = \"id\"  # used when saving predictions\n",
        "PERSON_COL = \"person_name\"  # optional for nicer output\n",
        "\n",
        "ARTIFACT_DIR = \"artifacts\"\n",
        "MODEL_OUT_DIR = os.path.join(ARTIFACT_DIR, \"model\")\n",
        "METRICS_PATH = os.path.join(ARTIFACT_DIR, \"metrics_validation.json\")\n",
        "REPORT_PATH = os.path.join(ARTIFACT_DIR, \"classification_report.txt\")\n",
        "LABELMAP_PATH = os.path.join(ARTIFACT_DIR, \"label_mapping.json\")\n",
        "PRED_OUT_PATH = os.path.join(ARTIFACT_DIR, \"test_set_with_sentiment.csv\")\n",
        "\n",
        "# Model choice\n",
        "MODEL_NAME = \"distilbert-base-uncased\"  # swap to \"roberta-base\" for RoBERTa\n",
        "MAX_LENGTH = 160\n",
        "N_EPOCHS = 3\n",
        "LR = 2e-5\n",
        "TRAIN_BS = 16\n",
        "EVAL_BS = 16\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.06\n",
        "SEED = 42\n",
        "\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "set_seed(SEED)\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "\n",
        "def coalesce_text(df: pd.DataFrame, primary: str, fallback: str) -> pd.Series:\n",
        "    \"\"\"Return a text series preferring primary, falling back to fallback, and filling NaNs.\"\"\"\n",
        "    prim = df[primary] if primary in df.columns else pd.Series([None]*len(df))\n",
        "    fall = df[fallback] if fallback in df.columns else pd.Series([None]*len(df))\n",
        "    # Choose primary if non-empty string after strip, else fallback\n",
        "    out = []\n",
        "    for a, b in zip(prim.fillna(\"\"), fall.fillna(\"\")):\n",
        "        at = str(a).strip()\n",
        "        bt = str(b).strip()\n",
        "        out.append(at if len(at) > 0 else bt)\n",
        "    return pd.Series(out)\n",
        "\n",
        "\n",
        "def normalize_label_series(s: pd.Series) -> (List[int], Dict[int, str], Dict[str, int]):\n",
        "    \"\"\"Convert a label series that may be numeric or string into ids 0..K-1.\n",
        "    Returns: (label_ids_list, id2label, label2id)\n",
        "    - If labels are numeric but not contiguous from 0, we remap to 0..K-1.\n",
        "    - If labels are strings, we sort label names for stable mapping.\n",
        "    \"\"\"\n",
        "    if s.dtype.kind in {\"i\", \"u\"}:\n",
        "        # numeric labels\n",
        "        uniq = sorted(pd.Series(s.unique()).dropna().tolist())\n",
        "        label2id = {str(lbl): i for i, lbl in enumerate(uniq)}\n",
        "        id2label = {i: str(lbl) for i, lbl in enumerate(uniq)}\n",
        "        mapped = [label2id[str(x)] for x in s]\n",
        "        return mapped, id2label, label2id\n",
        "    else:\n",
        "        # string labels\n",
        "        uniq = sorted(pd.Series(s.astype(str).unique()).dropna().tolist())\n",
        "        label2id = {lbl: i for i, lbl in enumerate(uniq)}\n",
        "        id2label = {i: lbl for lbl, i in label2id.items()}\n",
        "        mapped = [label2id[str(x)] for x in s.astype(str)]\n",
        "        return mapped, id2label, label2id\n",
        "\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    if labels is None: # Handle cases where labels are not available (e.g., test set prediction)\n",
        "        return {} # Return empty metrics\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
        "    return {\"accuracy\": acc, \"precision_macro\": precision, \"recall_macro\": recall, \"f1_macro\": f1}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Load data\n",
        "# -----------------------------\n",
        "print(\"Loading CSVs...\")\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "val_df = pd.read_csv(VAL_PATH)\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "\n",
        "# Build text series\n",
        "train_texts = coalesce_text(train_df, TEXT_COL_PRIMARY, TEXT_COL_FALLBACK)\n",
        "val_texts   = coalesce_text(val_df, TEXT_COL_PRIMARY, TEXT_COL_FALLBACK)\n",
        "test_texts  = coalesce_text(test_df, TEXT_COL_PRIMARY, TEXT_COL_FALLBACK)\n",
        "\n",
        "# Remove rows with truly empty text (rare but possible)\n",
        "train_mask = train_texts.str.len() > 0\n",
        "val_mask = val_texts.str.len() > 0\n",
        "\n",
        "train_df = train_df.loc[train_mask].reset_index(drop=True)\n",
        "val_df = val_df.loc[val_mask].reset_index(drop=True)\n",
        "train_texts = train_texts.loc[train_mask].reset_index(drop=True)\n",
        "val_texts = val_texts.loc[val_mask].reset_index(drop=True)\n",
        "\n",
        "# Normalize labels -> ids, and build mappings\n",
        "if LABEL_COL not in train_df.columns:\n",
        "    raise ValueError(f\"'{LABEL_COL}' column not found in training data.\")\n",
        "\n",
        "train_label_ids, id2label, label2id = normalize_label_series(train_df[LABEL_COL])\n",
        "val_label_ids, _, _ = normalize_label_series(val_df[LABEL_COL])\n",
        "\n",
        "# Persist mappings for downstream tasks\n",
        "with open(LABELMAP_PATH, \"w\") as f:\n",
        "    json.dump({\"id2label\": {str(k): v for k, v in id2label.items()},\n",
        "               \"label2id\": label2id}, f, indent=2)\n",
        "\n",
        "num_labels = len(id2label)\n",
        "print(f\"Detected {num_labels} labels: {id2label}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Tokenization\n",
        "# -----------------------------\n",
        "print(\"Tokenizing...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "train_enc = tokenizer(\n",
        "    train_texts.tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n",
        "val_enc = tokenizer(\n",
        "    val_texts.tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n",
        "test_enc = tokenizer(\n",
        "    test_texts.fillna(\"\").tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "train_dataset = TextDataset(train_enc, train_label_ids)\n",
        "val_dataset = TextDataset(val_enc, val_label_ids)\n",
        "test_dataset = TextDataset(test_enc, labels=None)\n",
        "\n",
        "# -----------------------------\n",
        "# Model & Training\n",
        "# -----------------------------\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=num_labels,\n",
        "    id2label={i: id2label[i] for i in range(num_labels)},\n",
        "    label2id={id2label[i]: i for i in range(num_labels)},\n",
        ")\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"CUDA available:\", use_cuda)\n",
        "\n",
        "warmup_steps = None  # we will use warmup_ratio instead\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(ARTIFACT_DIR, \"hf_runs\"),\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",  # Changed save_strategy to match eval_strategy\n",
        "    save_total_limit=N_EPOCHS,\n",
        "    save_steps=math.ceil(len(train_dataset) / TRAIN_BS),\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=TRAIN_BS,\n",
        "    per_device_eval_batch_size=EVAL_BS,\n",
        "    num_train_epochs=N_EPOCHS,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    fp16=use_cuda,  # enable mixed precision if GPU present\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Evaluating on validation set...\")\n",
        "val_metrics = trainer.evaluate()\n",
        "with open(METRICS_PATH, \"w\") as f:\n",
        "    json.dump(val_metrics, f, indent=2)\n",
        "print(\"Validation metrics saved to:\", METRICS_PATH)\n",
        "\n",
        "# Save a human-readable classification report\n",
        "val_logits = trainer.predict(val_dataset).predictions\n",
        "val_preds = np.argmax(val_logits, axis=-1)\n",
        "val_true = np.array(val_label_ids)\n",
        "report = classification_report(\n",
        "    val_true,\n",
        "    val_preds,\n",
        "    target_names=[id2label[i] for i in range(num_labels)],\n",
        "    digits=4,\n",
        ")\n",
        "with open(REPORT_PATH, \"w\") as f:\n",
        "    f.write(report)\n",
        "print(\"Classification report saved to:\", REPORT_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on test and save outputs\n",
        "# -----------------------------\n",
        "print(\"Predicting on test set...\")\n",
        "with torch.no_grad():\n",
        "    test_out = trainer.predict(test_dataset)\n",
        "    test_logits = test_out.predictions\n",
        "    test_pred_ids = np.argmax(test_logits, axis=-1)\n",
        "\n",
        "# Convert to label names\n",
        "id2label_str = {i: id2label[i] for i in range(num_labels)}\n",
        "test_labels = [id2label_str[i] for i in test_pred_ids]\n",
        "\n",
        "# Confidence (softmax)\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e / e.sum(axis=-1, keepdims=True)\n",
        "\n",
        "probs = softmax(test_logits)\n",
        "conf = probs.max(axis=-1)\n",
        "\n",
        "# Build output DataFrame\n",
        "out_df = pd.DataFrame({\n",
        "    ID_COL: test_df[ID_COL] if ID_COL in test_df.columns else np.arange(len(test_df)),\n",
        "    \"person_name\": test_df[PERSON_COL] if PERSON_COL in test_df.columns else None,\n",
        "    \"feedback_text\": test_texts,\n",
        "    \"predicted_sentiment\": test_labels,\n",
        "    \"pred_confidence\": conf,\n",
        "})\n",
        "\n",
        "out_df.to_csv(PRED_OUT_PATH, index=False)\n",
        "print(\"Saved test predictions to:\", PRED_OUT_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Save the best model & tokenizer\n",
        "# -----------------------------\n",
        "print(\"Saving model and tokenizer...\")\n",
        "trainer.save_model(MODEL_OUT_DIR)\n",
        "tokenizer.save_pretrained(MODEL_OUT_DIR)\n",
        "print(\"Model saved to:\", MODEL_OUT_DIR)\n",
        "\n",
        "print(\"All done. ✨\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "1moZr9Uh983S",
        "outputId": "9f58d636-b63b-40f1-941a-46648350264f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CSVs...\n",
            "Detected 9 labels: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8'}\n",
            "Tokenizing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "CUDA available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1984765913.py:251: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [123/123 00:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.129731</td>\n",
              "      <td>0.241379</td>\n",
              "      <td>0.098384</td>\n",
              "      <td>0.192519</td>\n",
              "      <td>0.120955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.163100</td>\n",
              "      <td>1.996161</td>\n",
              "      <td>0.370690</td>\n",
              "      <td>0.128056</td>\n",
              "      <td>0.291667</td>\n",
              "      <td>0.176755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.010900</td>\n",
              "      <td>1.924510</td>\n",
              "      <td>0.387931</td>\n",
              "      <td>0.175633</td>\n",
              "      <td>0.307540</td>\n",
              "      <td>0.207682</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on validation set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation metrics saved to: artifacts/metrics_validation.json\n",
            "Classification report saved to: artifacts/classification_report.txt\n",
            "Predicting on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved test predictions to: artifacts/test_set_with_sentiment.csv\n",
            "Saving model and tokenizer...\n",
            "Model saved to: artifacts/model\n",
            "All done. ✨\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\".\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxFUrBcpOB7V",
        "outputId": "30e90638-cc13-4259-8fa9-17076ea773cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'employee_review_mturk_dataset_test_v6_kaggle.csv', 'employee_review_mturk_dataset_v10_kaggle.csv', 'train_set.csv', 'test_set.csv', 'validation_set.csv', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# Task 1 – Transformer Sentiment (Clean, Weighted)\n",
        "# ==========================================\n",
        "# This version avoids modifying model.forward and instead uses a\n",
        "# custom Trainer (WeightedTrainer) to apply class-weighted loss.\n",
        "# Also increases max_length/epochs and includes robust label mapping.\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "TRAIN_PATH = \"train_set.csv\"\n",
        "VAL_PATH   = \"validation_set.csv\"\n",
        "TEST_PATH  = \"test_set.csv\"\n",
        "\n",
        "TEXT_COL_PRIMARY = \"feedback_clean\"   # preferred clean text\n",
        "TEXT_COL_FALLBACK = \"feedback\"        # fallback if clean text missing/empty\n",
        "LABEL_COL = \"label\"\n",
        "ID_COL = \"id\"\n",
        "PERSON_COL = \"person_name\"\n",
        "\n",
        "ARTIFACT_DIR = \"artifacts\"\n",
        "MODEL_OUT_DIR = os.path.join(ARTIFACT_DIR, \"model\")\n",
        "METRICS_PATH = os.path.join(ARTIFACT_DIR, \"metrics_validation.json\")\n",
        "REPORT_PATH = os.path.join(ARTIFACT_DIR, \"classification_report.txt\")\n",
        "LABELMAP_PATH = os.path.join(ARTIFACT_DIR, \"label_mapping.json\")\n",
        "PRED_OUT_PATH = os.path.join(ARTIFACT_DIR, \"test_set_with_sentiment.csv\")\n",
        "\n",
        "# Model choice\n",
        "MODEL_NAME = \"distilbert-base-uncased\"   # swap to \"roberta-base\" to try RoBERTa\n",
        "MAX_LENGTH = 256\n",
        "N_EPOCHS = 5\n",
        "LR = 3e-5\n",
        "TRAIN_BS = 16\n",
        "EVAL_BS = 16\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.06\n",
        "SEED = 42\n",
        "\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "set_seed(SEED)\n",
        "\n",
        "# -----------------------------\n",
        "# Safety check for common import conflict\n",
        "# -----------------------------\n",
        "# Warn if a local file/folder named 'torch' exists\n",
        "if any(name == \"torch.py\" or name == \"torch\" for name in os.listdir(\".\")):\n",
        "    print(\"[WARN] A local file/folder named 'torch' is present in the working directory.\\n\"\n",
        "          \"       This can shadow the real PyTorch package and cause AttributeError issues.\\n\"\n",
        "          \"       Rename it and remove __pycache__ if present.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "\n",
        "def coalesce_text(df: pd.DataFrame, primary: str, fallback: str) -> pd.Series:\n",
        "    prim = df[primary] if primary in df.columns else pd.Series([None]*len(df))\n",
        "    fall = df[fallback] if fallback in df.columns else pd.Series([None]*len(df))\n",
        "    out = []\n",
        "    for a, b in zip(prim.fillna(\"\"), fall.fillna(\"\")):\n",
        "        at = str(a).strip()\n",
        "        bt = str(b).strip()\n",
        "        out.append(at if len(at) > 0 else bt)\n",
        "    return pd.Series(out)\n",
        "\n",
        "\n",
        "def normalize_label_series(s: pd.Series) -> (List[int], Dict[int, str], Dict[str, int]):\n",
        "    # Convert possibly numeric/string labels into contiguous ids 0..K-1\n",
        "    if s.dtype.kind in {\"i\", \"u\"}:  # integer/unsigned\n",
        "        uniq = sorted(pd.Series(s.unique()).dropna().tolist())\n",
        "        label2id = {str(lbl): i for i, lbl in enumerate(uniq)}\n",
        "        id2label = {i: str(lbl) for i, lbl in enumerate(uniq)}\n",
        "        mapped = [label2id[str(x)] for x in s]\n",
        "    else:\n",
        "        uniq = sorted(pd.Series(s.astype(str).unique()).dropna().tolist())\n",
        "        label2id = {lbl: i for i, lbl in enumerate(uniq)}\n",
        "        id2label = {i: lbl for lbl, i in label2id.items()}\n",
        "        mapped = [label2id[str(x)] for x in s.astype(str)]\n",
        "    return mapped, id2label, label2id\n",
        "\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
        "    return {\"accuracy\": acc, \"precision_macro\": precision, \"recall_macro\": recall, \"f1_macro\": f1}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Load data\n",
        "# -----------------------------\n",
        "print(\"Loading CSVs...\")\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "val_df = pd.read_csv(VAL_PATH)\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "\n",
        "train_texts = coalesce_text(train_df, TEXT_COL_PRIMARY, TEXT_COL_FALLBACK)\n",
        "val_texts   = coalesce_text(val_df, TEXT_COL_PRIMARY, TEXT_COL_FALLBACK)\n",
        "test_texts  = coalesce_text(test_df, TEXT_COL_PRIMARY, TEXT_COL_FALLBACK)\n",
        "\n",
        "# Filter out truly empty\n",
        "train_mask = train_texts.str.len() > 0\n",
        "val_mask = val_texts.str.len() > 0\n",
        "train_df = train_df.loc[train_mask].reset_index(drop=True)\n",
        "val_df = val_df.loc[val_mask].reset_index(drop=True)\n",
        "train_texts = train_texts.loc[train_mask].reset_index(drop=True)\n",
        "val_texts = val_texts.loc[val_mask].reset_index(drop=True)\n",
        "\n",
        "if LABEL_COL not in train_df.columns:\n",
        "    raise ValueError(f\"'{LABEL_COL}' column not found in training data.\")\n",
        "\n",
        "train_label_ids, id2label, label2id = normalize_label_series(train_df[LABEL_COL])\n",
        "val_label_ids, _, _ = normalize_label_series(val_df[LABEL_COL])\n",
        "\n",
        "with open(LABELMAP_PATH, \"w\") as f:\n",
        "    json.dump({\"id2label\": {str(k): v for k, v in id2label.items()},\n",
        "               \"label2id\": label2id}, f, indent=2)\n",
        "\n",
        "num_labels = len(id2label)\n",
        "print(f\"Detected {num_labels} labels: {id2label}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Class imbalance handling\n",
        "# -----------------------------\n",
        "classes = np.unique(train_label_ids)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_label_ids)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# -----------------------------\n",
        "# Tokenization\n",
        "# -----------------------------\n",
        "print(\"Tokenizing...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "train_enc = tokenizer(train_texts.tolist(), padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "val_enc = tokenizer(val_texts.tolist(), padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "test_enc = tokenizer(test_texts.fillna(\"\").tolist(), padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "train_dataset = TextDataset(train_enc, train_label_ids)\n",
        "val_dataset = TextDataset(val_enc, val_label_ids)\n",
        "test_dataset = TextDataset(test_enc, labels=None)\n",
        "\n",
        "# -----------------------------\n",
        "# Model\n",
        "# -----------------------------\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=num_labels,\n",
        "    id2label={i: id2label[i] for i in range(num_labels)},\n",
        "    label2id={id2label[i]: i for i in range(num_labels)},\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Custom Trainer with weighted loss\n",
        "# -----------------------------\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # move weights to the right device lazily (Trainer sets device later)\n",
        "        self._class_weights_cpu = class_weights\n",
        "        self._class_weights = None\n",
        "\n",
        "    @property\n",
        "    def class_weights(self):\n",
        "        # ensure weights are on the same device as model\n",
        "        if self._class_weights is None or self._class_weights.device != self.args.device:\n",
        "            self._class_weights = self._class_weights_cpu.to(self.args.device)\n",
        "        return self._class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# -----------------------------\n",
        "# Training setup\n",
        "# -----------------------------\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"CUDA available:\", use_cuda)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(ARTIFACT_DIR, \"hf_runs\"),\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=TRAIN_BS,\n",
        "    per_device_eval_batch_size=EVAL_BS,\n",
        "    num_train_epochs=N_EPOCHS,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    fp16=use_cuda,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    class_weights=class_weights,\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Train & Evaluate\n",
        "# -----------------------------\n",
        "print(\"Training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Evaluating on validation set...\")\n",
        "val_metrics = trainer.evaluate()\n",
        "with open(METRICS_PATH, \"w\") as f:\n",
        "    json.dump(val_metrics, f, indent=2)\n",
        "print(\"Validation metrics saved to:\", METRICS_PATH)\n",
        "\n",
        "val_logits = trainer.predict(val_dataset).predictions\n",
        "val_preds = np.argmax(val_logits, axis=-1)\n",
        "val_true = np.array(val_label_ids)\n",
        "report = classification_report(val_true, val_preds, target_names=[id2label[i] for i in range(num_labels)], digits=4)\n",
        "with open(REPORT_PATH, \"w\") as f:\n",
        "    f.write(report)\n",
        "print(\"Classification report saved to:\", REPORT_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on test and save outputs\n",
        "# -----------------------------\n",
        "print(\"Predicting on test set...\")\n",
        "with torch.no_grad():\n",
        "    test_out = trainer.predict(test_dataset)\n",
        "    test_logits = test_out.predictions\n",
        "    test_pred_ids = np.argmax(test_logits, axis=-1)\n",
        "\n",
        "id2label_str = {i: id2label[i] for i in range(num_labels)}\n",
        "test_labels = [id2label_str[i] for i in test_pred_ids]\n",
        "\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e / e.sum(axis=-1, keepdims=True)\n",
        "\n",
        "probs = softmax(test_logits)\n",
        "conf = probs.max(axis=-1)\n",
        "\n",
        "out_df = pd.DataFrame({\n",
        "    ID_COL: test_df[ID_COL] if ID_COL in test_df.columns else np.arange(len(test_df)),\n",
        "    \"person_name\": test_df[PERSON_COL] if PERSON_COL in test_df.columns else None,\n",
        "    \"feedback_text\": test_texts,\n",
        "    \"predicted_sentiment\": test_labels,\n",
        "    \"pred_confidence\": conf,\n",
        "})\n",
        "\n",
        "out_df.to_csv(PRED_OUT_PATH, index=False)\n",
        "print(\"Saved test predictions to:\", PRED_OUT_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Save model & tokenizer\n",
        "# -----------------------------\n",
        "print(\"Saving model and tokenizer...\")\n",
        "trainer.save_model(MODEL_OUT_DIR)\n",
        "tokenizer.save_pretrained(MODEL_OUT_DIR)\n",
        "print(\"Model saved to:\", MODEL_OUT_DIR)\n",
        "\n",
        "print(\"All done. ✨\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwgw8rKrMqb1",
        "outputId": "509a0fd1-2604-41cb-8045-148968e2bbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CSVs...\n",
            "Detected 9 labels: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8'}\n",
            "Tokenizing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "print(nn)   # should print a module, not error\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vDECWGyN2aq",
        "outputId": "d80168f3-657a-4a9f-e0e0-cacf3bb0597e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'torch.nn' from '/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# Task 1 – Transformer Sentiment (Clean, Weighted)\n",
        "# ==========================================\n",
        "# This version avoids modifying model.forward and instead uses a\n",
        "# custom Trainer (WeightedTrainer) to apply class-weighted loss.\n",
        "# Also increases max_length/epochs and includes robust label mapping.\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "TRAIN_PATH = \"train_set.csv\"\n",
        "VAL_PATH   = \"validation_set.csv\"\n",
        "TEST_PATH  = \"test_set.csv\"\n",
        "\n",
        "TEXT_COL_PRIMARY = \"feedback_clean\"   # preferred clean text\n",
        "TEXT_COL_FALLBACK = \"feedback\"        # fallback if clean text missing/empty\n",
        "LABEL_COL = \"label\"\n",
        "ID_COL = \"id\"\n",
        "PERSON_COL = \"person_name\"\n",
        "\n",
        "ARTIFACT_DIR = \"artifacts\"\n",
        "MODEL_OUT_DIR = os.path.join(ARTIFACT_DIR, \"model\")\n",
        "METRICS_PATH = os.path.join(ARTIFACT_DIR, \"metrics_validation.json\")\n",
        "REPORT_PATH = os.path.join(ARTIFACT_DIR, \"classification_report.txt\")\n",
        "LABELMAP_PATH = os.path.join(ARTIFACT_DIR, \"label_mapping.json\")\n",
        "PRED_OUT_PATH = os.path.join(ARTIFACT_DIR, \"test_set_with_sentiment.csv\")\n",
        "\n",
        "# Model choice\n",
        "MODEL_NAME = \"distilbert-base-uncased\"   # swap to \"roberta-base\" to try RoBERTa\n",
        "MAX_LENGTH = 256\n",
        "N_EPOCHS = 5\n",
        "LR = 3e-5\n",
        "TRAIN_BS = 16\n",
        "EVAL_BS = 16\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.06\n",
        "SEED = 42\n",
        "\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "set_seed(SEED)\n",
        "\n",
        "# -----------------------------\n",
        "# Safety check for common import conflict\n",
        "# -----------------------------\n",
        "# Warn if a local file/folder named 'torch' exists\n",
        "if any(name == \"torch.py\" or name == \"torch\" for name in os.listdir(\".\")):\n",
        "    print(\"[WARN] A local file/folder named 'torch' is present in the working directory.\\n\"\n",
        "          \"       This can shadow the real PyTorch package and cause AttributeError issues.\\n\"\n",
        "          \"       Rename it and remove __pycache__ if present.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "\n",
        "def coalesce_text(df: pd.DataFrame, primary: str, fallback: str) -> pd.Series:\n",
        "    prim = df[primary] if primary in df.columns else pd.Series([None]*len(df))\n",
        "    fall = df[fallback] if fallback in df.columns else pd.Series([None]*len(df))\n",
        "    out = []\n",
        "    for a, b in zip(prim.fillna(\"\"), fall.fillna(\"\")):\n",
        "        at = str(a).strip()\n",
        "        bt = str(b).strip()\n",
        "        out.append(at if len(at) > 0 else bt)\n",
        "    return pd.Series(out)\n",
        "\n",
        "\n",
        "def normalize_label_series(s: pd.Series) -> (List[int], Dict[int, str], Dict[str, int]):\n",
        "    # Convert possibly numeric/string labels into contiguous ids 0..K-1\n",
        "    if s.dtype.kind in {\"i\", \"u\"}:  # integer/unsigned\n",
        "        uniq = sorted(pd.Series(s.unique()).dropna().tolist())\n",
        "        label2id = {str(lbl): i for i, lbl in enumerate(uniq)}\n",
        "        id2label = {i: str(lbl) for i, lbl in enumerate(uniq)}\n",
        "        mapped = [label2id[str(x)] for x in s]\n",
        "    else:\n",
        "        uniq = sorted(pd.Series(s.astype(str).unique()).dropna().tolist())\n",
        "        label2id = {lbl: i for i, lbl in enumerate(uniq)}\n",
        "        id2label = {i: lbl for lbl, i in label2id.items()}\n",
        "        mapped = [label2id[str(x)] for x in s.astype(str)]\n",
        "    return mapped, id2label, label2id\n",
        "\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
        "    return {\"accuracy\": acc, \"precision_macro\": precision, \"recall_macro\": recall, \"f1_macro\": f1}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Load data\n",
        "# -----------------------------\n",
        "print(\"Loading CSVs...\")\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "val_df = pd.read_csv(VAL_PATH)\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "\n",
        "train_texts = coalesce_text(train_df, TEXT_COL_PRIMARY, TEXT_COL_FALLBACK)\n",
        "val_texts   = coalesce_text(val_df, TEXT_COL_PRIMARY, TEXT_COL_FALLBACK)\n",
        "test_texts  = coalesce_text(test_df, TEXT_COL_PRIMARY, TEXT_COL_FALLBACK)\n",
        "\n",
        "# Filter out truly empty\n",
        "train_mask = train_texts.str.len() > 0\n",
        "val_mask = val_texts.str.len() > 0\n",
        "train_df = train_df.loc[train_mask].reset_index(drop=True)\n",
        "val_df = val_df.loc[val_mask].reset_index(drop=True)\n",
        "train_texts = train_texts.loc[train_mask].reset_index(drop=True)\n",
        "val_texts = val_texts.loc[val_mask].reset_index(drop=True)\n",
        "\n",
        "if LABEL_COL not in train_df.columns:\n",
        "    raise ValueError(f\"'{LABEL_COL}' column not found in training data.\")\n",
        "\n",
        "train_label_ids, id2label, label2id = normalize_label_series(train_df[LABEL_COL])\n",
        "val_label_ids, _, _ = normalize_label_series(val_df[LABEL_COL])\n",
        "\n",
        "with open(LABELMAP_PATH, \"w\") as f:\n",
        "    json.dump({\"id2label\": {str(k): v for k, v in id2label.items()},\n",
        "               \"label2id\": label2id}, f, indent=2)\n",
        "\n",
        "num_labels = len(id2label)\n",
        "print(f\"Detected {num_labels} labels: {id2label}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Class imbalance handling\n",
        "# -----------------------------\n",
        "classes = np.unique(train_label_ids)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_label_ids)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# -----------------------------\n",
        "# Tokenization\n",
        "# -----------------------------\n",
        "print(\"Tokenizing...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "train_enc = tokenizer(train_texts.tolist(), padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "val_enc = tokenizer(val_texts.tolist(), padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "test_enc = tokenizer(test_texts.fillna(\"\").tolist(), padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "train_dataset = TextDataset(train_enc, train_label_ids)\n",
        "val_dataset = TextDataset(val_enc, val_label_ids)\n",
        "test_dataset = TextDataset(test_enc, labels=None)\n",
        "\n",
        "# -----------------------------\n",
        "# Model\n",
        "# -----------------------------\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=num_labels,\n",
        "    id2label={i: id2label[i] for i in range(num_labels)},\n",
        "    label2id={id2label[i]: i for i in range(num_labels)},\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Custom Trainer with weighted loss\n",
        "# -----------------------------\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # move weights to the right device lazily (Trainer sets device later)\n",
        "        self._class_weights_cpu = class_weights\n",
        "        self._class_weights = None\n",
        "\n",
        "    @property\n",
        "    def class_weights(self):\n",
        "        # ensure weights are on the same device as model\n",
        "        if self._class_weights is None or self._class_weights.device != self.args.device:\n",
        "            self._class_weights = self._class_weights_cpu.to(self.args.device)\n",
        "        return self._class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=0, ignore_index=-100):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=self.class_weights, ignore_index=ignore_index)\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# -----------------------------\n",
        "# Training setup\n",
        "# -----------------------------\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"CUDA available:\", use_cuda)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(ARTIFACT_DIR, \"hf_runs\"),\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=TRAIN_BS,\n",
        "    per_device_eval_batch_size=EVAL_BS,\n",
        "    num_train_epochs=N_EPOCHS,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    fp16=use_cuda,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    class_weights=class_weights,\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Train & Evaluate\n",
        "# -----------------------------\n",
        "print(\"Training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Evaluating on validation set...\")\n",
        "val_metrics = trainer.evaluate()\n",
        "with open(METRICS_PATH, \"w\") as f:\n",
        "    json.dump(val_metrics, f, indent=2)\n",
        "print(\"Validation metrics saved to:\", METRICS_PATH)\n",
        "\n",
        "val_logits = trainer.predict(val_dataset).predictions\n",
        "val_preds = np.argmax(val_logits, axis=-1)\n",
        "val_true = np.array(val_label_ids)\n",
        "report = classification_report(val_true, val_preds, target_names=[id2label[i] for i in range(num_labels)], digits=4)\n",
        "with open(REPORT_PATH, \"w\") as f:\n",
        "    f.write(report)\n",
        "print(\"Classification report saved to:\", REPORT_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on test and save outputs\n",
        "# -----------------------------\n",
        "print(\"Predicting on test set...\")\n",
        "with torch.no_grad():\n",
        "    test_out = trainer.predict(test_dataset)\n",
        "    test_logits = test_out.predictions\n",
        "    test_pred_ids = np.argmax(test_logits, axis=-1)\n",
        "\n",
        "id2label_str = {i: id2label[i] for i in range(num_labels)}\n",
        "test_labels = [id2label_str[i] for i in test_pred_ids]\n",
        "\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e / e.sum(axis=-1, keepdims=True)\n",
        "\n",
        "probs = softmax(test_logits)\n",
        "conf = probs.max(axis=-1)\n",
        "\n",
        "out_df = pd.DataFrame({\n",
        "    ID_COL: test_df[ID_COL] if ID_COL in test_df.columns else np.arange(len(test_df)),\n",
        "    \"person_name\": test_df[PERSON_COL] if PERSON_COL in test_df.columns else None,\n",
        "    \"feedback_text\": test_texts,\n",
        "    \"predicted_sentiment\": test_labels,\n",
        "    \"pred_confidence\": conf,\n",
        "})\n",
        "\n",
        "out_df.to_csv(PRED_OUT_PATH, index=False)\n",
        "print(\"Saved test predictions to:\", PRED_OUT_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Save model & tokenizer\n",
        "# -----------------------------\n",
        "print(\"Saving model and tokenizer...\")\n",
        "trainer.save_model(MODEL_OUT_DIR)\n",
        "tokenizer.save_pretrained(MODEL_OUT_DIR)\n",
        "print(\"Model saved to:\", MODEL_OUT_DIR)\n",
        "\n",
        "print(\"All done. ✨\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "EgiyxVmRPDry",
        "outputId": "4e593412-db04-4cae-e6d4-e1ca857d19f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CSVs...\n",
            "Detected 9 labels: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8'}\n",
            "Tokenizing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "CUDA available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-591221758.py:196: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='205' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [205/205 00:35, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.158556</td>\n",
              "      <td>0.181034</td>\n",
              "      <td>0.092637</td>\n",
              "      <td>0.174587</td>\n",
              "      <td>0.114343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.186400</td>\n",
              "      <td>1.904523</td>\n",
              "      <td>0.362069</td>\n",
              "      <td>0.221558</td>\n",
              "      <td>0.292253</td>\n",
              "      <td>0.210120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.951300</td>\n",
              "      <td>1.748572</td>\n",
              "      <td>0.396552</td>\n",
              "      <td>0.302408</td>\n",
              "      <td>0.353257</td>\n",
              "      <td>0.316883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.656600</td>\n",
              "      <td>1.632641</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.400522</td>\n",
              "      <td>0.449431</td>\n",
              "      <td>0.413893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.456200</td>\n",
              "      <td>1.615832</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.452933</td>\n",
              "      <td>0.437322</td>\n",
              "      <td>0.419690</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on validation set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation metrics saved to: artifacts/metrics_validation.json\n",
            "Classification report saved to: artifacts/classification_report.txt\n",
            "Predicting on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved test predictions to: artifacts/test_set_with_sentiment.csv\n",
            "Saving model and tokenizer...\n",
            "Model saved to: artifacts/model\n",
            "All done. ✨\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 – Exploratory Data Analysis (EDA)\n",
        "# ========================================\n",
        "# This script performs EDA on the employee sentiment dataset.\n",
        "# Input: train_set.csv, validation_set.csv, test_set_with_sentiment.csv\n",
        "# Output: visualizations (saved to artifacts/eda/) + printed insights\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# -----------------------------\n",
        "# Setup\n",
        "# -----------------------------\n",
        "ARTIFACT_DIR = \"artifacts/eda\"\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_PATH = \"train_set.csv\"\n",
        "VAL_PATH   = \"validation_set.csv\"\n",
        "TEST_PATH  = \"test_set_with_sentiment.csv\"\n",
        "\n",
        "# -----------------------------\n",
        "# Load Data\n",
        "# -----------------------------\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "val = pd.read_csv(VAL_PATH)\n",
        "test = pd.read_csv(TEST_PATH)\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Validation shape:\", val.shape)\n",
        "print(\"Test shape:\", test.shape)\n",
        "\n",
        "print(\"\\nTrain head:\")\n",
        "print(train.head())\n",
        "\n",
        "# -----------------------------\n",
        "# Missing values check\n",
        "# -----------------------------\n",
        "print(\"\\nMissing values (train):\")\n",
        "print(train.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values (test):\")\n",
        "print(test.isnull().sum())\n",
        "\n",
        "# -----------------------------\n",
        "# Sentiment Distribution\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=train['label'])\n",
        "plt.title(\"Training Set Sentiment Distribution\")\n",
        "plt.savefig(os.path.join(ARTIFACT_DIR, \"train_sentiment_distribution.png\"))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=test['predicted_sentiment'])\n",
        "plt.title(\"Test Set Predicted Sentiment Distribution\")\n",
        "plt.savefig(os.path.join(ARTIFACT_DIR, \"test_sentiment_distribution.png\"))\n",
        "plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Sentiment Over Time (if date column exists)\n",
        "# -----------------------------\n",
        "# Check if 'reviewed' column exists and is suitable for datetime conversion in the test set\n",
        "if 'reviewed' in test.columns and test['reviewed'].dtype != 'object':\n",
        "    try:\n",
        "        test['reviewed'] = pd.to_datetime(test['reviewed'])\n",
        "        monthly_sentiment = test.groupby([test['reviewed'].dt.to_period(\"M\"), 'predicted_sentiment']).size().unstack().fillna(0)\n",
        "\n",
        "        if not monthly_sentiment.empty:\n",
        "            monthly_sentiment.plot(kind='line', marker='o', figsize=(10,6))\n",
        "            plt.title(\"Monthly Sentiment Trends\")\n",
        "            plt.ylabel(\"Message Count\")\n",
        "            plt.xlabel(\"Month\")\n",
        "            plt.savefig(os.path.join(ARTIFACT_DIR, \"monthly_sentiment_trends.png\"))\n",
        "            plt.close()\n",
        "        else:\n",
        "            print(\"\\nNo valid date data in 'reviewed' column of test set for time series plot.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCould not create monthly sentiment plot: {e}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Employee-wise Analysis\n",
        "# -----------------------------\n",
        "if 'person_name' in test.columns:\n",
        "    employee_sentiment = test.groupby(['person_name','predicted_sentiment']).size().unstack().fillna(0)\n",
        "\n",
        "    # Top 10 employees by message count\n",
        "    top10 = employee_sentiment.sum(axis=1).sort_values(ascending=False).head(10)\n",
        "    if not top10.empty:\n",
        "        plt.figure(figsize=(10,6))\n",
        "        top10.plot(kind='bar')\n",
        "        plt.title(\"Top 10 Employees by Number of Messages\")\n",
        "        plt.ylabel(\"Message Count\")\n",
        "        plt.savefig(os.path.join(ARTIFACT_DIR, \"top10_employees.png\"))\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(\"\\nNo data for employee-wise analysis.\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Wordclouds\n",
        "# -----------------------------\n",
        "if 'feedback_text' in test.columns:\n",
        "    # Get all unique predicted sentiment labels from the test set\n",
        "    sentiment_labels = test['predicted_sentiment'].unique()\n",
        "\n",
        "    # Generate a wordcloud for each sentiment label\n",
        "    for label in sentiment_labels:\n",
        "        text_for_label = \" \".join(test[test['predicted_sentiment']==label][\"feedback_text\"].dropna())\n",
        "\n",
        "        if text_for_label: # Only generate wordcloud if there is text for the label\n",
        "            plt.figure(figsize=(8, 4))\n",
        "            wordcloud = WordCloud(width=800, height=400).generate(text_for_label)\n",
        "            plt.imshow(wordcloud, interpolation='bilinear')\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"WordCloud for Sentiment: {label}\")\n",
        "            plt.savefig(os.path.join(ARTIFACT_DIR, f\"wordcloud_sentiment_{label}.png\"))\n",
        "            plt.close()\n",
        "        else:\n",
        "            print(f\"\\nNo text found for sentiment label: {label}. Skipping wordcloud generation.\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Summary Printouts\n",
        "# -----------------------------\n",
        "print(\"\\n=== EDA Insights ===\")\n",
        "print(\"Training set sentiment distribution:\")\n",
        "print(train['label'].value_counts(normalize=True))\n",
        "\n",
        "print(\"Test set predicted sentiment distribution:\")\n",
        "print(test['predicted_sentiment'].value_counts(normalize=True))\n",
        "\n",
        "if 'person_name' in test.columns and 'top10' in locals():\n",
        "    print(\"Top employees by number of messages:\")\n",
        "    print(top10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTd9E0P6Q1O5",
        "outputId": "a143bf99-d47b-4fb0-c3ec-9a958b313b60"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (656, 13)\n",
            "Validation shape: (116, 13)\n",
            "Test shape: (225, 5)\n",
            "\n",
            "Train head:\n",
            "      id     person_name                                  nine_box_category  \\\n",
            "0      1        John Doe  Category 1: 'Risk' (Low performance, Low poten...   \n",
            "1  10045   Douglas Henry  Category 1: 'Risk' (Low performance, Low poten...   \n",
            "2  10044   Douglas Henry  Category 1: 'Risk' (Low performance, Low poten...   \n",
            "3  10005  Freddie Davies  Category 1: 'Risk' (Low performance, Low poten...   \n",
            "4  10004  Freddie Davies  Category 1: 'Risk' (Low performance, Low poten...   \n",
            "\n",
            "                                            feedback  adjusted  reviewed  \\\n",
            "0  John has not progressed in his position. He is...     False      True   \n",
            "1  Douglas Henry has been having trouble in all a...     False     False   \n",
            "2  Douglas has a lot to work on and areas to grow...     False     False   \n",
            "3  Freddie is a nice guy, but his performance and...     False      True   \n",
            "4  Freddie has been quite disappointing this quar...     False     False   \n",
            "\n",
            "   label  feedback_len  num_of_sent  performance_class  potential_class  \\\n",
            "0      0           287            5                  0                0   \n",
            "1      0           430            6                  0                0   \n",
            "2      0           290            4                  0                0   \n",
            "3      0           418            5                  0                0   \n",
            "4      0           449            4                  0                0   \n",
            "\n",
            "                                      feedback_clean data_type  \n",
            "0  john ha not progressed in his position he is c...     train  \n",
            "1  douglas henry ha been having trouble in all ar...     train  \n",
            "2  douglas ha a lot to work on and area to grow i...     train  \n",
            "3  freddie is a nice guy but his performance and ...     train  \n",
            "4  freddie ha been quite disappointing this quart...     train  \n",
            "\n",
            "Missing values (train):\n",
            "id                   0\n",
            "person_name          0\n",
            "nine_box_category    0\n",
            "feedback             0\n",
            "adjusted             0\n",
            "reviewed             0\n",
            "label                0\n",
            "feedback_len         0\n",
            "num_of_sent          0\n",
            "performance_class    0\n",
            "potential_class      0\n",
            "feedback_clean       0\n",
            "data_type            0\n",
            "dtype: int64\n",
            "\n",
            "Missing values (test):\n",
            "id                     0\n",
            "person_name            0\n",
            "feedback_text          0\n",
            "predicted_sentiment    0\n",
            "pred_confidence        0\n",
            "dtype: int64\n",
            "\n",
            "=== EDA Insights ===\n",
            "Training set sentiment distribution:\n",
            "label\n",
            "0    0.147866\n",
            "4    0.134146\n",
            "8    0.132622\n",
            "3    0.121951\n",
            "1    0.109756\n",
            "2    0.106707\n",
            "5    0.100610\n",
            "7    0.099085\n",
            "6    0.047256\n",
            "Name: proportion, dtype: float64\n",
            "Test set predicted sentiment distribution:\n",
            "predicted_sentiment\n",
            "4    0.364444\n",
            "0    0.271111\n",
            "8    0.257778\n",
            "3    0.075556\n",
            "2    0.031111\n",
            "Name: proportion, dtype: float64\n",
            "Top employees by number of messages:\n",
            "person_name\n",
            "Amy Jones         5.0\n",
            "Aubri Hartman     5.0\n",
            "Aydin Pitts       5.0\n",
            "Layne Terrell     5.0\n",
            "Jack Walsh        5.0\n",
            "Georgia Rogers    5.0\n",
            "Georgia King      5.0\n",
            "Ella Green        5.0\n",
            "Bryan Murray      5.0\n",
            "Callen Bentley    5.0\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 3 – Employee Score Calculation (no dates available)\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "ARTIFACT_DIR = \"artifacts/task3\"\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "\n",
        "# Load labeled dataset\n",
        "test = pd.read_csv(\"test_set_with_sentiment.csv\")\n",
        "\n",
        "# Map sentiments to numeric scores\n",
        "sentiment_map = {\"Positive\": 1, \"Negative\": -1, \"Neutral\": 0}\n",
        "test['sentiment_score'] = test['predicted_sentiment'].map(sentiment_map)\n",
        "\n",
        "# Aggregate scores by employee (no monthly grouping possible)\n",
        "employee_scores = (\n",
        "    test.groupby('person_name')['sentiment_score']\n",
        "    .sum()\n",
        "    .reset_index()\n",
        "    .sort_values(by='sentiment_score', ascending=False)\n",
        ")\n",
        "\n",
        "# Save output\n",
        "employee_scores.to_csv(os.path.join(ARTIFACT_DIR, \"employee_scores.csv\"), index=False)\n",
        "\n",
        "print(\"Employee sentiment scores saved to artifacts/task3/employee_scores.csv\")\n",
        "print(employee_scores.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Jzn6dtsRwfV",
        "outputId": "f9cac115-0d74-408b-d085-033bf7340013"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee sentiment scores saved to artifacts/task3/employee_scores.csv\n",
            "         person_name  sentiment_score\n",
            "0        Alisa Stark              0.0\n",
            "1        Allan Logan              0.0\n",
            "2          Amy Jones              0.0\n",
            "3       Andrew Grant              0.0\n",
            "4  Angelica Peterson              0.0\n",
            "5      Archie Dawson              0.0\n",
            "6     Aryanna Carney              0.0\n",
            "7        Ashton Owen              0.0\n",
            "8      Aubri Hartman              0.0\n",
            "9        Aydin Pitts              0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 – Employee Ranking\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "ARTIFACT_DIR = \"artifacts/task4\"\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "\n",
        "# Try loading monthly scores first, else fall back to overall scores\n",
        "monthly_path = \"artifacts/task3/employee_monthly_scores.csv\"\n",
        "overall_path = \"artifacts/task3/employee_scores.csv\"\n",
        "\n",
        "if os.path.exists(monthly_path):\n",
        "    print(\"Using monthly scores...\")\n",
        "    df = pd.read_csv(monthly_path)\n",
        "\n",
        "    # Add ranking per month\n",
        "    df['rank'] = df.groupby('year_month')['sentiment_score'].rank(\n",
        "        method=\"dense\", ascending=False\n",
        "    ).astype(int)\n",
        "\n",
        "    out_path = os.path.join(ARTIFACT_DIR, \"employee_monthly_ranking.csv\")\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"Monthly ranking saved to {out_path}\")\n",
        "    print(df.head(10))\n",
        "\n",
        "elif os.path.exists(overall_path):\n",
        "    print(\"Using overall scores...\")\n",
        "    df = pd.read_csv(overall_path)\n",
        "\n",
        "    # Add global rank\n",
        "    df['rank'] = df['sentiment_score'].rank(\n",
        "        method=\"dense\", ascending=False\n",
        "    ).astype(int)\n",
        "\n",
        "    out_path = os.path.join(ARTIFACT_DIR, \"employee_ranking.csv\")\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"Overall ranking saved to {out_path}\")\n",
        "    print(df.head(10))\n",
        "\n",
        "else:\n",
        "    raise FileNotFoundError(\"Neither employee_monthly_scores.csv nor employee_scores.csv found. Please run Task 3 first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTKRenFrUJEk",
        "outputId": "dfbb4196-319f-4d12-91b2-ca7e05585bba"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using monthly scores...\n",
            "Monthly ranking saved to artifacts/task4/employee_monthly_ranking.csv\n",
            "Empty DataFrame\n",
            "Columns: [person_name, year_month, sentiment_score, rank]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 – Flight Risk Identification\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "ARTIFACT_DIR = \"artifacts/task5\"\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "\n",
        "monthly_path = \"artifacts/task4/employee_monthly_ranking.csv\"\n",
        "overall_path = \"artifacts/task4/employee_ranking.csv\"\n",
        "\n",
        "def classify_risk(score, rank, rank_percentile):\n",
        "    if score < 0 or rank_percentile >= 0.8:\n",
        "        return \"High Risk\"\n",
        "    elif score == 0 or (0.4 <= rank_percentile < 0.8):\n",
        "        return \"Medium Risk\"\n",
        "    else:\n",
        "        return \"Low Risk\"\n",
        "\n",
        "if os.path.exists(monthly_path):\n",
        "    print(\"Using monthly ranking for risk analysis...\")\n",
        "    df = pd.read_csv(monthly_path)\n",
        "\n",
        "    # Compute average score and average rank per employee\n",
        "    summary = df.groupby('person_name').agg(\n",
        "        avg_score=('sentiment_score','mean'),\n",
        "        avg_rank=('rank','mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Normalize ranks into percentile\n",
        "    summary['rank_percentile'] = summary['avg_rank'] / summary['avg_rank'].max()\n",
        "\n",
        "    # Assign risk\n",
        "    summary['flight_risk'] = summary.apply(\n",
        "        lambda row: classify_risk(row['avg_score'], row['avg_rank'], row['rank_percentile']), axis=1\n",
        "    )\n",
        "\n",
        "    out_path = os.path.join(ARTIFACT_DIR, \"employee_flight_risk.csv\")\n",
        "    summary.to_csv(out_path, index=False)\n",
        "    print(f\"Flight risk file saved to {out_path}\")\n",
        "    print(summary.head(10))\n",
        "\n",
        "elif os.path.exists(overall_path):\n",
        "    print(\"Using overall ranking for risk analysis...\")\n",
        "    df = pd.read_csv(overall_path)\n",
        "\n",
        "    df['rank_percentile'] = df['rank'] / df['rank'].max()\n",
        "    df['flight_risk'] = df.apply(\n",
        "        lambda row: classify_risk(row['sentiment_score'], row['rank'], row['rank_percentile']), axis=1\n",
        "    )\n",
        "\n",
        "    out_path = os.path.join(ARTIFACT_DIR, \"employee_flight_risk.csv\")\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"Flight risk file saved to {out_path}\")\n",
        "    print(df.head(10))\n",
        "\n",
        "else:\n",
        "    raise FileNotFoundError(\"No ranking data found from Task 4. Please run Task 4 first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLrtgy_eUZum",
        "outputId": "dcd091c1-caaa-4eeb-ad5d-50e7183a0d33"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using monthly ranking for risk analysis...\n",
            "Flight risk file saved to artifacts/task5/employee_flight_risk.csv\n",
            "Empty DataFrame\n",
            "Columns: [person_name, avg_score, avg_rank, rank_percentile, flight_risk]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 – Streamlit Dashboard\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "st.set_page_config(page_title=\"Employee Sentiment Dashboard\", layout=\"wide\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load Data\n",
        "# -----------------------------\n",
        "sentiment_path = \"test_set_with_sentiment.csv\"\n",
        "score_path = \"artifacts/task3/employee_scores.csv\"\n",
        "monthly_score_path = \"artifacts/task3/employee_monthly_scores.csv\"\n",
        "ranking_path = \"artifacts/task4/employee_ranking.csv\"\n",
        "monthly_ranking_path = \"artifacts/task4/employee_monthly_ranking.csv\"\n",
        "risk_path = \"artifacts/task5/employee_flight_risk.csv\"\n",
        "\n",
        "@st.cache_data\n",
        "def load_data(path):\n",
        "    if os.path.exists(path):\n",
        "        return pd.read_csv(path)\n",
        "    return None\n",
        "\n",
        "test = load_data(sentiment_path)\n",
        "scores = load_data(score_path)\n",
        "monthly_scores = load_data(monthly_score_path)\n",
        "ranking = load_data(ranking_path)\n",
        "monthly_ranking = load_data(monthly_ranking_path)\n",
        "risk = load_data(risk_path)\n",
        "\n",
        "st.title(\"📊 Employee Sentiment & Flight Risk Dashboard\")\n",
        "\n",
        "# -----------------------------\n",
        "# Sentiment Distribution\n",
        "# -----------------------------\n",
        "st.header(\"1. Sentiment Distribution\")\n",
        "if test is not None:\n",
        "    fig, ax = plt.subplots(figsize=(6,4))\n",
        "    sns.countplot(x=test['predicted_sentiment'], ax=ax)\n",
        "    ax.set_title(\"Predicted Sentiment Distribution\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "# -----------------------------\n",
        "# Sentiment Trends Over Time\n",
        "# -----------------------------\n",
        "st.header(\"2. Sentiment Trends Over Time\")\n",
        "if monthly_scores is not None and \"year_month\" in monthly_scores.columns:\n",
        "    trend = monthly_scores.groupby(['year_month'])['sentiment_score'].sum().reset_index()\n",
        "    fig, ax = plt.subplots(figsize=(8,4))\n",
        "    sns.lineplot(data=trend, x=\"year_month\", y=\"sentiment_score\", marker=\"o\", ax=ax)\n",
        "    ax.set_title(\"Monthly Sentiment Trends\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "# -----------------------------\n",
        "# Employee Scores\n",
        "# -----------------------------\n",
        "st.header(\"3. Employee Sentiment Scores\")\n",
        "if scores is not None:\n",
        "    st.dataframe(scores)\n",
        "    fig, ax = plt.subplots(figsize=(8,4))\n",
        "    top_scores = scores.sort_values(by=\"sentiment_score\", ascending=False).head(10)\n",
        "    sns.barplot(data=top_scores, x=\"sentiment_score\", y=\"person_name\", ax=ax)\n",
        "    ax.set_title(\"Top 10 Employees by Sentiment Score\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "# -----------------------------\n",
        "# Employee Ranking\n",
        "# -----------------------------\n",
        "st.header(\"4. Employee Rankings\")\n",
        "if ranking is not None:\n",
        "    st.dataframe(ranking.sort_values(by=\"rank\"))\n",
        "elif monthly_ranking is not None:\n",
        "    st.dataframe(monthly_ranking.head(20))\n",
        "\n",
        "# -----------------------------\n",
        "# Flight Risk\n",
        "# -----------------------------\n",
        "st.header(\"5. Flight Risk Identification\")\n",
        "if risk is not None:\n",
        "    st.dataframe(risk)\n",
        "    fig, ax = plt.subplots(figsize=(6,4))\n",
        "    sns.countplot(x=risk['flight_risk'], ax=ax, order=[\"Low Risk\", \"Medium Risk\", \"High Risk\"])\n",
        "    ax.set_title(\"Distribution of Flight Risk Levels\")\n",
        "    st.pyplot(fig)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w4z_OnIW0HG",
        "outputId": "c5568a82-aad2-4bcc-d747-b367e6803da4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-19 17:24:47.283 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.296 No runtime found, using MemoryCacheStorageManager\n",
            "2025-08-19 17:24:47.319 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.320 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.322 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.325 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.331 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.333 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.487 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.935 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.944 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.949 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.952 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:47.953 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.044 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.322 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.324 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.326 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.327 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.331 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.332 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.338 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.340 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.446 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.904 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.910 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.913 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.915 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.922 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.925 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.928 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.929 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.933 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.934 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.936 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.942 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.945 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.946 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:48.976 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:49.274 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:49.281 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-19 17:24:49.282 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    }
  ]
}